{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b177ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# node_edge_equivalence.py\n",
    "#\n",
    "# Verify that a FlexAttention rewrite of NodeEdgeBlock is numerically\n",
    "# identical to the hand-rolled implementation.\n",
    "\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.attention.flex_attention import flex_attention\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Helper layers                                                               #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def masked_softmax_dim2(scores: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Soft-max over dim=2 with â€“inf masking. Accepts (bs,n,n,h,df) or (bs,n,n,h).\"\"\"\n",
    "    if scores.dim() == 5:                      # (bs,n_q,n_k,h,df)\n",
    "        scores = scores.sum(-1)                # -> (bs,n,n,h)\n",
    "    scores = scores.masked_fill(mask == 0, -1e30)\n",
    "    return torch.softmax(scores, dim=2)\n",
    "\n",
    "\n",
    "class Xtoy(nn.Module):\n",
    "    def __init__(self, dx, dy):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dx, dy)\n",
    "\n",
    "    def forward(self, X, x_mask):\n",
    "        summed = (X * x_mask).sum(1)\n",
    "        denom  = x_mask.sum(1).clamp_min(1e-6)\n",
    "        return self.lin(summed / denom)\n",
    "\n",
    "\n",
    "class Etoy(nn.Module):\n",
    "    def __init__(self, de, dy):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(de, dy)\n",
    "\n",
    "    def forward(self, E, e_mask1, e_mask2):\n",
    "        mask   = e_mask1 * e_mask2\n",
    "        summed = (E * mask).sum((1, 2))\n",
    "        denom  = mask.sum((1, 2)).clamp_min(1e-6)\n",
    "        return self.lin(summed / denom)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 1.  Original block                                                          #\n",
    "# --------------------------------------------------------------------------- #\n",
    "class NodeEdgeBlock(nn.Module):\n",
    "    def __init__(self, dx, de, dy, n_head):\n",
    "        super().__init__()\n",
    "        assert dx % n_head == 0\n",
    "        self.dx, self.de, self.dy = dx, de, dy\n",
    "        self.n_head, self.df      = n_head, dx // n_head\n",
    "\n",
    "        # projections\n",
    "        self.q = nn.Linear(dx, dx)\n",
    "        self.k = nn.Linear(dx, dx)\n",
    "        self.v = nn.Linear(dx, dx)\n",
    "\n",
    "        self.e_mul, self.e_add = nn.Linear(de, dx), nn.Linear(de, dx)\n",
    "        self.y_e_mul, self.y_e_add = nn.Linear(dy, dx), nn.Linear(dy, dx)\n",
    "        self.y_x_mul, self.y_x_add = nn.Linear(dy, dx), nn.Linear(dy, dx)\n",
    "\n",
    "        self.y_y  = nn.Linear(dy, dy)\n",
    "        self.x_y  = Xtoy(dx, dy)\n",
    "        self.e_y  = Etoy(de, dy)\n",
    "\n",
    "        self.x_out = nn.Linear(dx, dx)\n",
    "        self.e_out = nn.Linear(dx, de)\n",
    "        self.y_out = nn.Sequential(nn.Linear(dy, dy), nn.ReLU(), nn.Linear(dy, dy))\n",
    "\n",
    "    # --------------------------------------------------------------------- #\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        bs, n, _ = X.shape\n",
    "        x_mask  = node_mask.unsqueeze(-1)          # (bs,n,1)\n",
    "        e_mask1 = x_mask.unsqueeze(2)              # (bs,n,1,1)\n",
    "        e_mask2 = x_mask.unsqueeze(1)              # (bs,1,n,1)\n",
    "\n",
    "        # -------- Q, K ---------------------------------------------------- #\n",
    "        Q = self.q(X) * x_mask\n",
    "        K = self.k(X) * x_mask\n",
    "        Qv = Q.view(bs, n, self.n_head, self.df)   # (bs,n,h,df)\n",
    "        Kv = K.view(bs, n, self.n_head, self.df)\n",
    "\n",
    "        # full vector scores\n",
    "        Y = (Qv.unsqueeze(2) * Kv.unsqueeze(1)) / math.sqrt(self.df)  # (bs,n,n,h,df)\n",
    "\n",
    "        E1 = (self.e_mul(E) * e_mask1 * e_mask2).view(bs, n, n, self.n_head, self.df)\n",
    "        E2 = (self.e_add(E) * e_mask1 * e_mask2).view_as(E1)\n",
    "        Y  = Y * (E1 + 1) + E2                         # (bs,n,n,h,df)\n",
    "\n",
    "        # -------- edge update -------------------------------------------- #\n",
    "        newE = Y.flatten(3)\n",
    "        ye1, ye2 = self.y_e_add(y)[:, None, None, :], self.y_e_mul(y)[:, None, None, :]\n",
    "        newE = self.e_out(ye1 + (ye2 + 1) * newE) * e_mask1 * e_mask2\n",
    "\n",
    "        # -------- attention scalars -------------------------------------- #\n",
    "        soft_mask = e_mask2.expand(-1, n, -1, self.n_head)\n",
    "        attn = masked_softmax_dim2(Y, soft_mask)      # (bs,n,n,h)\n",
    "\n",
    "        # -------- value --------------------------------------------------- #\n",
    "        V = self.v(X) * x_mask                        # (bs,n,dx)\n",
    "        V = V.view(bs, n, self.n_head, self.df).unsqueeze(1)  # (bs,1,n,h,df)\n",
    "        weighted = (attn.unsqueeze(-1) * V).sum(2)    # (bs,n,h,df)\n",
    "        weighted = weighted.flatten(2)                # (bs,n,dx)\n",
    "\n",
    "        # -------- node update -------------------------------------------- #\n",
    "        yx1, yx2 = self.y_x_add(y)[:, None, :], self.y_x_mul(y)[:, None, :]\n",
    "        newX = self.x_out(yx1 + (yx2 + 1) * weighted) * x_mask\n",
    "\n",
    "        # -------- global update ------------------------------------------ #\n",
    "        new_y = self.y_out(self.y_y(y) + self.x_y(X, x_mask) + self.e_y(E, e_mask1, e_mask2))\n",
    "        return newX, newE, new_y\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 2.  FlexAttention version                                                   #\n",
    "# --------------------------------------------------------------------------- #\n",
    "class NodeEdgeBlockFlex(NodeEdgeBlock):\n",
    "    \"\"\"Same math as NodeEdgeBlock but with a fused Flash/Flex kernel.\"\"\"\n",
    "    def __init__(self, dx, de, dy, n_head):\n",
    "        super().__init__(dx, de, dy, n_head)\n",
    "        self.sqrt_df = math.sqrt(self.df)\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        bs, n, _ = X.shape\n",
    "        x_mask  = node_mask.unsqueeze(-1)\n",
    "        e_mask1 = x_mask.unsqueeze(2)\n",
    "        e_mask2 = x_mask.unsqueeze(1)\n",
    "\n",
    "        # -------- projections -------------------------------------------- #\n",
    "        Q = self.q(X) * x_mask                 # (bs,n,dx)\n",
    "        K = self.k(X) * x_mask\n",
    "        V = self.v(X) * x_mask\n",
    "\n",
    "        Qv = Q.view(bs, n, self.n_head, self.df)   # (bs,n,h,df)\n",
    "        Kv = K.view(bs, n, self.n_head, self.df)\n",
    "        Vv = V.view(bs, n, self.n_head, self.df)\n",
    "\n",
    "        # same Y as reference for edge update\n",
    "        Y = (Qv.unsqueeze(2) * Kv.unsqueeze(1)) / self.sqrt_df\n",
    "\n",
    "        E1 = (self.e_mul(E) * e_mask1 * e_mask2).view(bs, n, n, self.n_head, self.df)\n",
    "        E2 = (self.e_add(E) * e_mask1 * e_mask2).view_as(E1)\n",
    "        Y  = Y * (E1 + 1) + E2                        # (bs,n,n,h,df)\n",
    "\n",
    "        # -------- edge update identical ---------------------------------- #\n",
    "        newE = Y.flatten(3)\n",
    "        newE = self.e_out(self.y_e_add(y)[:, None, None, :] +\n",
    "                          (self.y_e_mul(y)[:, None, None, :] + 1) * newE) * e_mask1 * e_mask2\n",
    "\n",
    "        # -------- FlexAttention fusion ----------------------------------- #\n",
    "        # Layout: (bs, h, seq, df)\n",
    "        Q_flex = Qv.transpose(1, 2)       # (bs,h,n,df)\n",
    "        K_flex = Kv.transpose(1, 2)\n",
    "        V_flex = Vv.transpose(1, 2)\n",
    "\n",
    "        # Closure tensors must already live on the right device/dtype\n",
    "        E1_flat = E1                      # (bs,n,n,h,df)\n",
    "        E2_flat = E2\n",
    "\n",
    "        key_padding = (node_mask == 0)    # True = ignore\n",
    "\n",
    "        def score_mod(_score, b, h, q, k):\n",
    "            \"\"\"Exact scalar = Î£_df [ (q_d k_d / âˆšdf)*(1+E1) + E2 ]\"\"\"\n",
    "            q_vec = Qv[b, q, h]           # (df)\n",
    "            k_vec = Kv[b, k, h]\n",
    "            base  = q_vec * k_vec / self.sqrt_df\n",
    "            return (base * (E1_flat[b, q, k, h] + 1) +\n",
    "                    E2_flat[b, q, k, h]).sum(-1)\n",
    "\n",
    "        attn_V = flex_attention(\n",
    "            Q_flex, K_flex, V_flex,\n",
    "            score_mod=score_mod,\n",
    "            key_padding_mask=key_padding\n",
    "        )                                 # (bs,h,n,df)\n",
    "\n",
    "        weighted = attn_V.transpose(1, 2).contiguous()  # (bs,n,h,df)\n",
    "        weighted = weighted.flatten(2)                  # (bs,n,dx)\n",
    "\n",
    "        # -------- node + global updates as before ------------------------ #\n",
    "        newX = self.x_out(self.y_x_add(y)[:, None, :] +\n",
    "                          (self.y_x_mul(y)[:, None, :] + 1) * weighted) * x_mask\n",
    "        new_y = self.y_out(self.y_y(y) + self.x_y(X, x_mask) + self.e_y(E, e_mask1, e_mask2))\n",
    "        return newX, newE, new_y\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 3.  Sanity check                                                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def _run_equivalence():\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    bs, n      = 3, 5\n",
    "    dx, de, dy = 32, 16, 8\n",
    "    n_head     = 8\n",
    "\n",
    "    X  = torch.randn(bs, n, dx)\n",
    "    E  = torch.randn(bs, n, n, de)\n",
    "    y  = torch.randn(bs, dy)\n",
    "    mask = (torch.rand(bs, n) > 0.3).float()\n",
    "\n",
    "    ref  = NodeEdgeBlock(dx, de, dy, n_head)\n",
    "    flex = NodeEdgeBlockFlex(dx, de, dy, n_head)\n",
    "    flex.load_state_dict(ref.state_dict())           # identical weights\n",
    "\n",
    "    ref.eval(); flex.eval()\n",
    "    with torch.no_grad():\n",
    "        x0, e0, y0 = ref (X, E, y, mask)\n",
    "        x1, e1, y1 = flex(X, E, y, mask)\n",
    "\n",
    "    def err(a, b): return (a - b).abs().max().item()\n",
    "    print(f\"max |Î”X| = {err(x0,x1):.3e}\")\n",
    "    print(f\"max |Î”E| = {err(e0,e1):.3e}\")\n",
    "    print(f\"max |Î”y| = {err(y0,y1):.3e}\")\n",
    "\n",
    "    tol = 1e-5\n",
    "    assert err(x0,x1) < tol and err(e0,e1) < tol and err(y0,y1) < tol\n",
    "    print(\"ðŸŽ‰  FlexAttention block matches the original within\", tol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536909e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "flex_attention() got an unexpected keyword argument 'key_padding_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43m_run_equivalence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 211\u001b[39m, in \u001b[36m_run_equivalence\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    210\u001b[39m     x0, e0, y0 = ref (X, E, y, mask)\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     x1, e1, y1 = \u001b[43mflex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merr\u001b[39m(a, b): \u001b[38;5;28;01mreturn\u001b[39;00m (a - b).abs().max().item()\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmax |Î”X| = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr(x0,x1)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DiffMS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DiffMS/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 173\u001b[39m, in \u001b[36mNodeEdgeBlockFlex.forward\u001b[39m\u001b[34m(self, X, E, y, node_mask)\u001b[39m\n\u001b[32m    169\u001b[39m     base  = q_vec * k_vec / \u001b[38;5;28mself\u001b[39m.sqrt_df\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (base * (E1_flat[b, q, k, h] + \u001b[32m1\u001b[39m) +\n\u001b[32m    171\u001b[39m             E2_flat[b, q, k, h]).sum(-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m attn_V = \u001b[43mflex_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mQ_flex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_flex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_flex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m                                 \u001b[38;5;66;03m# (bs,h,n,df)\u001b[39;00m\n\u001b[32m    179\u001b[39m weighted = attn_V.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()  \u001b[38;5;66;03m# (bs,n,h,df)\u001b[39;00m\n\u001b[32m    180\u001b[39m weighted = weighted.flatten(\u001b[32m2\u001b[39m)                  \u001b[38;5;66;03m# (bs,n,dx)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: flex_attention() got an unexpected keyword argument 'key_padding_mask'"
     ]
    }
   ],
   "source": [
    "_run_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24ef41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
